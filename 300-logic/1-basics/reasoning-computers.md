# Logic :: About :: Reasoning computers

Having a formal symbolic language, like the Leibniz's universal language, instead of dealing with a natural language, would make it possible to program it into a computer, perhaps even to teach computers how to reason based on it.

Computers, at least still today (2023), have no intelligence and can only perform task by following a series of precise steps that we program in explicitly. The instructions have to be extremely precise because computers essentialy start with a blank slate - unlike humans, they have no a priori knowledge nor they have any pressupositions, so any knowledge that we may impart on them can never be considered trivial or commonplace (by us, humans). This is dificult not only because we don't even know ourselves much about how the brain works, conscience and reasoning, but also because it's hard to entirely rid oneself of pressupositions and prejudice.


Logic, in the myriad of its versions and variants, is the means towards such a goal. Being able to formalize a natural language is also beneficial for computer to human interaction and communication, computer learning, and for teaching computers how to reason logically, among many other possibilities. This would be like a universal language that Leibniz envisioned,

In any case, logic remains as the best tool for this, as it fills the goal of having a small core (GPT3 is trained on anything but a small amount of data). By further advancing *automated reasoning* we hope to get an advanced kind of *query processing*. *Predicate logic* is a good fit for having a small core, especially since *logical programming languages*, like `Prolog`, have already demonstrated good approaches and usable techics.


However, we don't have to shoot all the way for reasoning computers; having computers that can assists us sensibly in browsing through the wastness of human knowledge and making computer-assisted queries would do nicely for now. Since it is impossible to make all human knowledge available to computers directly - to store it as a ginormous database of facts - it would be desirable to instead store only a small core of facts and then have computers make new inferences starting with that core. That wey we could get away with a far more tractable task; we would particularly avoid storing the knowledge that can be easily inferred. Instead of storing the entire argument, we can store just the premises and let it derive the conculsions. With every conclusion it derives, its knowledge database grows, so there are more and more facts for making future inferences more precise.

A computer can only ever know that part of the entirety of human knowledge that it was thought explicitly or that it derived from the previous body of knowledge that started with the core set of facts. Setting aside what would that core set of facts be, the thing of interest to logic is the formal language that would be used for this interaction.

Whenever we face a complex problem like this one, we can do what we humans do best: *analysis*, *abstraction* and *synthesis*. We first decompose one big-ass problem into a set of the smallest possible tractable subproblems. We then study the parts meticulously, slowely abstracting them into larger units as we comprehend them more and more, until we are eventually able to recreate the original problem. This is *reductivism*, a view that we can understand a whole as the sum of its parts. It doesn't always apply or work, but it always pays to give it a go.

A supplementary goal that would make teaching computers easier is to get them to understand natural language. Thanks to the recent advances in that area, we now have a *GPT3 Chat* (2023) algorithm that seems great at understanding humans. GPT3 is trained by studying enourmous amounts of human communication, but it cannot really be said that it posses true understanding of what it generates, it is probably more true that it is an excellent copycat, excelling in interpretation and generation of plausable and, sufficiently correct, resonses to queries. It does brings the goal of having a database of all human knowledge and having copmuters answer and assists us in querying it. The other goal - being able to make new inferences from the existing facts - is not yet achieved, and GPT wasn't even designed with that task in mind ⟨I think; also I've witnessed that it does understand written language extremely well and produces adequate responses, but the YouTube concesus seems to be that it is far from an intelligent agent. As I understand it, it can generate new text, as a response to a query, but that text is - and the facts in it - it must have had already encountered before; that is, it cannot make new inferences. But the industry seems to be getting there fast enough⟩.
