# Logic :: Fundamentals :: 3. Argumentation

The central concept around which logic develops is that of reasoning. Actually, logic leaves general reasoning to philosophy and other sciences, itself being most concerned with a particular type of reasoning. *Logical reasoning* allows us to derive new information from the existing knowledge, but in a way that is rigorous and logically consistant. The general *logical form* that the content subject to correct logical reasoning takes is called an *argument*.

An **inference** is any new information extracted from the information you already have.

>An **argument** is the general logical form that the content subject to correct logical reasoning takes.

Argument is the content subject to reasoning, and the process is called **argumentation** or **inference**. Inference happens in the mind and through various forms of communication, but it is modelled in the written form as a collection of sentences.

However, not any collection or any type of sentence makes a correct argument. Only *declarative sentences* structurally organized in two groups form a correct argument. One groups of statements, called *premises*, are claims that lend support (evidence, justification, reason, grounds) for the final statement, called the *conclusion*.

>An argument consists of a set of declarative sentences called **premises**, which serve as the support for the final statement, called a **conclusion**.

The type of sentences that make an argument are declarative sentences; they are also called statements, claims, assertions, but most of all **propositions**, especially in this role. However, not even all declarative sentences are suitable as propositions; among problematic instances are statements that talk about future events or statements open to interpretations.

Linguistics classifies sentences according to their functionality into *interrogative*, *excalamative*, *imperative* and *declarative* sentences. Declarative sentences are the only suitable type for argumentation because they express facts and we can scrutinize these facts. Unlike the other types, we can talk about the veracity of declarative sentences (i.e. propositions) by presenting our reasons as to why we think they are true or false.

The property of being true or false means having a *truth value*; true and false are two truth values. Since only declarative sentences have a truth value, they are the only type of sentences we can argue about, as people often do. A verbal form of such argumentation is called a *debate* and it takes the form of a verbal exchange between *interlocutors*.

>*Inference* is the process of deriving a conclusion from a set of premises.

A new fact, in the form of a conclusion, is derived from the information contained in a set of premises. Logic identifies the exact manner in which *inference* - the process of going from premises to a conclusion - is performed.

Logic examines and appraises correct arguments with the aim to codify the correct ways of logical reasoning. But in analysing logical reasoning, logic distinguishes even between different *types of logical reasoning*. There are different types of reasoning in general, but it also turns out there are different types of logical reasoning too (which we investigate next).

---

The essential point is that new statements are derived from existing ones by "interactions" that implement **laws of inference**, e.g. that Q can be derived from the statement P, and the statement that P implies Q (modus ponens).

If we trace the paths by which one statement can be derived from others, these correspond to proofs. And the whole graph of all these derivations is then a representation of the possible historical development of mathematics - with slices through this graph corresponding to the sets of statements reached at a given stage.

But down at the lowest level, is there some specific computational rule that's "running the universe"? I don't think so. Instead, I think that in effect all possible rules are always being applied. And the result is the ruliad: the entangled structure associated with performing all possible computations.

In the traditional formulation of axiomatic mathematics, one talks about deriving results from particular axiom systems - say Peano Arithmetic, or ZFC set theory, or the axioms of Euclidean geometry. But the ruliad in effect represents the entangled consequences not just of specific axiom systems but of all possible axiom systems (as well as all possible laws of inference).

---

Monadic pure predicate logic - in which predicates always take only a single argument - reduces in effect to basic logic and is not universal. But as soon as there is even one arbitrary predicate with two arguments the system becomes universal. And indeed this is the case even if one considers only statements with quantifiers ∀ ∃. (The system is also universal with one binary function or two unary functions). In basic logic any statement that is true for all possible assignments of truth values to variables can always be proved from the axioms of basic logic. In 1930 Kurt Gödel showed a similar result for pure predicate logic: that any statement that is true for all possible explicit values of variables and all possible forms of predicates can always be proved from the axioms of predicate logic (This is often called *Gödel's Completeness Theorem*, but is not related to completeness).

---

Chapter 1: The Foundations for a New Kind of Science
- Section 1: An Outline of Basic Ideas

## The role of logic

In addition to standard mathematics, the formal system most widely discussed since antiquity is logic (see page 1099). And starting with Aristotle there was in fact a long tradition of trying to use logic as a framework for drawing conclusions about nature. In the early 1600s the experimental method was suggested as a better alternative. And after mathematics began to show extensive success in describing nature in the late 1600s no further large-scale efforts to do this on the basis of logic appear to have been made. It is conceivable that Gottfried Leibniz might have tried in the late 1600s, but when his work was followed up in the late 1800s by Gottlob Frege and others the emphasis was on building up mathematics, not natural science, from logic (see page 1149). And indeed by this point logic was viewed mostly as a possible representation of human thought-and not as a formal system relevant to nature. So when computers arose it was their numerical and mathematical rather than logical capabilities that were normally assumed relevant for natural science. But in the early 1980s the cellular automata that I studied I often characterized as being based on logical rules, rather than traditional mathematical ones. However, as we will see on page 806, traditional logic is in fact in many ways very narrow compared to the whole range of rules based on simple programs that I actually consider in this book.

Chapter 10: Processes of Perception and Analysis
- Section 12: Human Thinking


# History of ideas about thinking

https://www.wolframscience.com/nks/notes-10-12--history-of-ideas-about-thinking/

Ever since antiquity immense amounts have been written about human *thinking*.

Until recent centuries, most of it was in the tradition of philosophy, and indeed one of the major themes of philosophy throughout its history has been the elucidation of principles of human thinking.

However, almost all the relevant ideas generated have remained forever controversial, and almost none have become concrete enough to be applied in science or technology.

An exception is *logic*, which was introduced in earnest by *Aristotle* in the 4th century BC as a way *to model certain patterns of human reasoning*.

Logic developed somewhat in *medieval times*, and in the late 1600s Gottfried *Leibniz* tried to use it as the foundation for a *universal language* to capture all *systematic thinking*. 

Beginning with the work of George *Boole* in the mid-1800s most of logic began to become more closely integrated with mathematics and even less convincingly relevant as a model for general human thinking.

The notion of applying scientific methods to the study of human thinking developed largely with the rise of the field of *psychology* in the mid-1800s.

Two somewhat different approaches were taken. 
* The first concentrated on doing fairly controlled experiments on humans or animals and looking at responses to specific stimuli.
* The second concentrated on trying to formulate fairly general theories based on observations of overall human behavior, initially in adults and later especially in children. 

Both approaches achieved some success, but by the 1930s many of their positions had become quite extreme, and the identification of phenomena to contradict every simple conclusion reached led increasingly to the view that human thinking would allow no simple explanations.

The idea that it might be possible to construct *machines that could emulate human thinking* existed already in antiquity, and became increasingly popular starting in the 1600s. It began to appear widely in fiction in the 1800s, and has remained a standard fixture in portrayals of the future ever since.

In the early 1900s, it became clear that the brain consists of *neurons* which operate electrically, and by the 1940s analogies between brains and electrical machines were widely discussed, particularly in the context of the *cybernetics* movement. 

In 1943, Warren McCulloch and Walter Pitts formulated a simple idealized model of networks of neurons and tried to analyze it using methods of mathematical logic. 

In 1949, Donald Hebb then argued that simple underlying neural mechanisms could explain observed psychological phenomena such as learning. 

Computer simulations of *neural networks* were done starting in the mid-1950s, but the networks were too small to have any chance to exhibit behavior that could reasonably be identified with thinking. (Ironically enough, the phenomenon central to this book of *complex behavior with simple underlying rules* was in effect seen in some of these experiments, but it was considered a distraction and ignored).

And in the 1960s, particularly after Frank Rosenblatt's introduction of *perceptrons*, neural networks were increasingly used only as systems for specific visual and other tasks.

The idea that computers could be made to exhibit human-like thinking was discussed by Alan *Turing* in 1950 using many of the same arguments that one would give today. Turing made the prediction that by 2000 a computer would exist that could pass the so-called *Turing test* and be able to imitate a human in a conversation. (René Descartes had discussed a similar test for machines in 1637, but concluded that it would never be passed.) 

When electronic computers were first becoming widespread in the 1950s they were often popularly referred to as "electronic brains". And when early efforts to make computers perform tasks such as playing games were fairly successful, the expectation developed that general human-like thinking was not far away. 

In the 1960s, with extensive support from the U.S. government, great effort was put into the field of *artificial intelligence*. Many programs were written to perform specific tasks. Sometimes the programs were set up to follow general models of the high-level processes of thinking. 

But by the 1970s it was becoming clear that in almost all cases where programs were successful (notable examples being chess, algebra and autonomous control), they typically worked by following definite algorithms not closely related to general human thinking.

Occasional work on neural networks had continued through the 1960s and 1970s, with a few definite results being obtained using methods from physics. 

Then in the early 1980s, particularly following work by John Hopfield, computer simulations of *neural networks* became widespread. Early applications, particularly by Terrence Sejnowski and Geoffrey Hinton, demonstrated that simple neural networks could be made to learn tasks of at least some sophistication. 

But by the mid-1990s it was becoming clear that - probably in large part as a consequence of reliance on methods from traditional mathematics - typical neural network models were mostly being successful only in situations where what was needed was a fairly straightforward extension of standard continuous probabilistic models of data.
