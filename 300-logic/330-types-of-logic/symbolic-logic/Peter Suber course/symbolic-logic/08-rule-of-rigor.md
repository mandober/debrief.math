---
downloaded:       2022-06-20
page-url:         https://web.archive.org/web/19990506173757/http://www.earlham.edu/~peters/courses/log/rigor.htm
article-title:    Peter Suber, "The Rule of Rigor"
---
# Peter Suber, "The Rule of Rigor"
The Wayback Machine - https://web.archive.org/web/19990506173757/http://www.earlham.edu:80/%7Epeters/courses/log/rigor.htm

> **Justifying the Rule of Rigor**
> 
> [Peter Suber][1], [Philosophy Department][2], [Earlham College][3]
> 
> In the first half of the course, or so, we will follow the rule of rigor for derivations. To see what this means, look at how Copi presents the rule of simplification:
> 
> p · q
> 
>  p   
> 
> The rule of rigor means that from "p · q" we can infer only the left-hand conjunct "p" by the rule of simplification. To infer the right-hand conjunct "q" we must first put "q" on the left, using the rule of commutation, and then apply the rule of simplification.
> 
> (On exams, you may derive "q" in one step provided you list two justifications for it, namely, commutation and simplification. If you do it in one step with only one justification, you violate the rule of rigor and will lose some credit, though only a small amount.)
> 
> The rule of rigor forces us to take more steps in order to spell out the obvious. Therefore it is a nuisance. What can justify it?
> 
> 1.  When human beings skip steps in their reasoning, they are prone to make mistakes. Taking every step and no short-cuts will reduce the chance of error. It will also cultivate our logical intuition, so that we may thereafter skip steps with less chance of error. At some point, then, we will relax the rule of rigor.
> 2.  Skipping no steps vividly shows us that reasoning can be reduced to step-taking. (Or: this kind of reasoning can be reduced to step-taking.) When each step is small enough to require no intelligence, then this means that reasoning (this kind of reasoning) can be mechanized. This fact is of supreme importance for the theory of computation. And this is one reason we study logic: to learn how to program reasoning for machines to carry out.
> 3.  I assured you on the first day of class that you already know how to reason in the basic ways studied in this class. What we are learning in the course is the formal structure of that reasoning, which is analogous to learning the grammar of your native language. As formal knowledge of grammar, it is new (and perhaps difficult); but as your native language, it is very familiar. Your impatience with the rule of rigor proves this point. You reason so fluently that you can skip steps without giving up intuitive obviousness. Inferences that are intuitive and immediate for you still have some internal structure that our notation, terminology, and rules can explicate. One value of this course is to make the structure of that reasoning explicit, even if in practice you don't use your explicit knowledge. The fact that what you do easily in one step can be explicated and justified logically only in several steps is less a nuisance than a commentary on the fine-grained detail of our logical apparatus and the fluency of your natural reasoning. The rule of rigor, then, teaches us much about our natural reasoning, not just about the formal reasoning which can be programmed.
> 4.  Actually, the previous paragraph is misleading. The number of steps in an inference does not depend on the inference, but on our decision to recognize some rules of inference and not others. We could add another rule to our toolkit, say, *right-conjunct simplification* that would allow us to get "q" from "p · q" in one step. The inference would take one step in the enlarged toolkit, two in our current toolkit. Your fluency in natural reasoning is the same either way, but with one set of rules we model that natural reasoning with 20 rules, and with the other we use 19. We could add rules like right-conjunct simplification whenever the rule of rigor got in our way, although that would be pretty silly. The new rules would not extend the deductive power of our rule-set in any way at all, and would only enhance its intuitive completeness in microscopically small ways. Moreover, there are good reasons to keep the rule-set relatively small. For example, one psychological purpose of proof is to reduce a complex, non-intuitive inference to a series of steps that are each simple and intuitive. With our small rule-set, we need only work on making 19 rules familiar and intuitive. Even if we were willing to add rules like right-conjunct simplification, would that succeed in eliminating the nuisance of the rule of rigor? It's possible that in any finite set of rules, no matter how large, the rule of rigor will show some obvious inferences to take several steps and the nuisance would survive. But that's speculation. Let's be logical: either adding new rules would eliminate the nuisance or it would not. If it would, then we would eliminate one nuisance only at the price of adding another, namely, an unmanageably huge set of rules. If it would not eliminate nuisance, then we could only eliminate the nuisance by ceasing to follow rules, which is to give up logic.
> 5.  One motivation to develop the kind of logic we are studying in this course is to clarify our intuitive notion of proof. In the late 19th and early 20th centuries, mathematicians discovered paradoxes at the foundations of set theory and some branches of logic. Since the rest of mathematics could be construed to rest on these fields, the possibility that mathematics itself was paradoxical or self-contradictory was open and had to be explored. Mathematicians differed on methods by which they attacked this problem, but they agreed that the concept of proof in mathematics had to be made much more rigorous and precise, and that proofs had to be conducted according to a much higher standard of rigor. We do logic and mathematics in the wake of this aroused vigilance. We are free to adopt the rules of our choice, but once we adopt them, there are very good reasons to follow them rigorously.
> 6.  We've seen that validity is formal or syntactical; now we are seeing a related fact, that rule-following is also formal or syntactical. (The *strategy* for deciding which rules to apply is quite distinct from rule-following in this sense.) Syntax knows nothing of intelligent shortcuts or intuitive leaps that would violate the rule of rigor. An unintelligent machine could follow a rule syntactically by looking for the appropriate marks on the page (say, "p · q"), then converting them to another set of marks (say, "p"). To follow the rule of rigor is to see how conducting a proof with rigor is like a machine executing a program. This is not a mere analogy but an isomorphism. Once we omit the content of an argument by symbolizing its logical structure, drawing inferences becomes isomorphic with running software. That is, every proof is a kind of computation, and *vice versa*. Logical expressions composed of symbols correspond to bytes of bits. Premises correspond to input. Rules of inference correspond to the functions in a program. Conclusions correspond to output. Proof corresponds to computation. Hence, limits on provability correspond to limits on computability. The rule of rigor corresponds to the fact that, at the lowest level, computing machines perform only unintelligent syntactical operations and never "decide for themselves" that steps are so obvious that they can be skipped. This brings clarity to both logic and computation.

---

This file is an electronic hand-out for the course, [Symbolic Logic][4].

Some of the logic symbols in this file are GIFs. See my [Notes on Logic Notation on the Web][5].

[][6] [Peter Suber][7], [Department of Philosophy][8], [Earlham College][9], Richmond, Indiana, 47374, U.S.A.  
[*peters@earlham.edu*][10]. [Copyright][11] © 1997, Peter Suber.

[1]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~peters/hometoc.htm
[2]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~phil/index.htm
[3]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/
[4]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~peters/courses/log/loghome.htm
[5]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~peters/writing/logicsym.htm
[6]: https://web.archive.org/web/19990506173757/http://www.eff.org/blueribbon.html
[7]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~peters/hometoc.htm
[8]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~phil/index.htm
[9]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/
[10]: https://web.archive.org/web/19990506173757/mailto:peters@earlham.edu
[11]: https://web.archive.org/web/19990506173757/http://www.earlham.edu/~peters/copyrite.htm
