---
downloaded:       2022-01-22
page-url:         https://plato.stanford.edu/archives/fall2014/entries/logical-consequence/
page-title:       Logical Consequence (Stanford Encyclopedia of Philosophy/Fall 2014 Edition)
article-title:    Logical Consequence
---
# Logical Consequence (Stanford Encyclopedia of Philosophy/Fall 2014 Edition)

Some arguments are such that the (joint) truth of the premises is
necessarily sufficient for the truth of the conclusions. In
the sense of logical consequence central to the current
tradition, such “necessary sufficiency” distinguishes
deductive validity from inductive validity. In inductively
valid arguments, the (joint) truth of the premises is very likely
(but not necessarily) sufficient for the truth of the conclusion.
An inductively valid argument is such that, as it is often put, its
premises make its conclusion more likely or more reasonable (even
though the conclusion may well be untrue given the joint truth of the
premises). The argument
## 1\. Deductive and Inductive Consequence

Some arguments are such that the (joint) truth of the premises is *necessarily sufficient* for the truth of the conclusions. In the sense of *logical consequence* central to the current tradition, such “necessary sufficiency” distinguishes deductive validity from *inductive* validity. In inductively valid arguments, the (joint) truth of the premises is *very likely (but not necessarily) sufficient* for the truth of the conclusion. An inductively valid argument is such that, as it is often put, its premises make its conclusion more likely or more reasonable (even though the conclusion may well be untrue given the joint truth of the premises). The argument

2.  All swans observed so far have been white.  
    Smoothy is a swan.  
    Therefore, Smoothy is white.

is not deductively valid because the premises are not necessarily sufficient for the conclusion. Smoothy may well be a black swan.

Distinctions can be drawn between different inductive arguments. Some inductive arguments seem quite reasonable, and others are less so. There are many different ways to attempt to analyse inductive consequence. We might consider the degree to which the premises make the conclusion *more likely* (a probabilistic reading), or we might check whether the most *normal* circumstances in which the premises are true render the conclusion true as well. (This leads to some kinds of default or non-monotonic inference.) The field of inductive consequence is difficult and important, but we shall leave that topic here and focus on *deductive* validity.

(See the entries on [inductive logic][1] and [non-monotonic logic][2] for more information on these topics.)

The constraint of *necessity* is not sufficient to settle the notion of deductive validity, for the notion of *necessity* may also be fleshed out in a number of ways. To say that a conclusion necessarily follows from the premises is to say that the argument is somehow *exceptionless*, but there are many different ways to make that idea precise.

A first stab at the notion might use what we now call metaphysical necessity. Perhaps an argument is valid if it is *(metaphysically) impossible* for the premises to be true and the conclusion to be untrue, valid if—holding fixed the interpretations of premises and conclusion—in every possible world in which the premises hold, so does the conclusion. This constraint is plausibly thought to be a necessary condition for logical consequence (if it *could be* that the premises are true and the conclusion isn't, then there is no doubt that the conclusion does not follow from the premises); however, on most accounts of logical consequence, it is not a sufficient condition for validity. Many admit the existence of *a posteriori* necessities, such as the claim that water is H2O. If that claim is necessary, then the argument:

3.  *x* is water.  
    Therefore, *x* is H2O.

is necessarily truth preserving, but it seems a long way from being deductively valid. It was a genuine discovery that water is H2O, one that required significant empirical investigation. While there may be genuine discoveries of valid arguments that we had not previously recognised as such, it is another thing entirely to think that these discoveries require empirical investigation.

An alternative line on the requisite sort of *necessity* turns to *conceptual necessity*. On this line, the conclusion of (3) is not a consequence of its premise given that it is not a conceptual truth that water is H2O. The concept *water* and the concept *H2O* happen to pick out the same property, but this agreement is determined partially by the world.

A similar picture of logic takes consequence to be a matter of what is *analytically* true, and it is not an analytic truth that water is H2O. The word “water” and the formula “H2O” agree in extension (and necessarily so) but they do not agree in *meaning*.

If metaphysical necessity is too coarse a notion to determine logical consequence (since it may be taken to render too many arguments deductively valid), an appeal to conceptual or analytic necessity might seem to be a better route. The trouble, as Quine argued, is that the distinction between analytic and synthetic (and similarly, conceptual and non-conceptual) truths is not as straightforward as we might have thought in the beginning of the 20th Century. (See the entry on the [analytic/synthetic distinction][3].) Furthermore many arguments seem to be truth-preserving on the basis of analysis alone:

4.  Peter is Greg's mother's brother's son.  
    Therefore, Peter is Greg's cousin.

One can understand that the conclusion follows from the premises, on the basis of one's understanding of the concepts involved. One need not know anything about the identity of Peter, Greg's cousin. Still, many have thought that (4) is not deductively valid, despite its credentials as truth-preserving on analytic or conceptual grounds. It is not quite as general as it could be because it is not as *formal* as it could be. The argument succeeds only because of the particular details of family concepts involved.

A further possibility for carving out the distinctive notion of *necessity* grounding logical consequence is the notion of *apriority*. Deductively valid arguments, whatever they are, can be known to be so without recourse to experience, so they must be knowable *a priori*. A constraint of apriority certainly seems to rule argument (3) out as deductively valid, and rightly so. However, it will not do to rule out argument (4). If we take arguments like (4) to turn not on matters of deductive validity but something else, such as an *a priori* knowable definition, then we must look elsewhere for a characterisation of logical consequence.

## 2\. Formal and Material Consequence

The strongest and most widespread proposal for finding a narrower criterion for logical consequence is the appeal to *formality*. The step in (4) from “Peter is Greg's mother's brother's son” to “Peter is my cousin” is a *material* consequence and not a formal one, because to make the step from the premise to the conclusion we need more than the *structure* or *form* of the claims involved: we need to understand their *contents* too.

What could the distinction between *form* and *content* mean? We mean to say that consequence is formal if it depends on the *form* and not the *substance* of the claims involved. But how is that to be understood? We will give at most a sketch, which, again, can be filled out in a number of ways.

The obvious first step is to notice that all presentations of the *rules* of logical consequence rely on [schemes][4]. Aristotle's syllogistic is a proud example.

> F*e*r*io*: No *F* is *G*. Some *H* is *G*. Therefore some *H* is not *F*.

Inference schemes, like the one above, display the structure of valid arguments. Perhaps to say that an argument is formally valid is to say that it falls under some general scheme of which every instance is valid, such as F*e*r*io*.

That, too, is an incomplete specification of formality. The material argument (4) is an instance of:

5.  *x* is *y*'s mother's brother's son.  
    Therefore, *x* is *y*'s cousin.

every instance of which is valid. We must say more to explain why some schemes count as properly formal (and hence a sufficient ground for logical consequence) and others do not. A general answer will articulate the notion of [logical form][5], which is an important issue in its own right (involving the notion of [logical constants][6], among other things). Instead of exploring the details of different candidates for logical form, we will mention different proposals about the *point* of the exercise.

What is the point in demanding that validity be underwritten by a notion of logical form? There are at least three distinct proposals for the required notion of formality, and each provides a different kind of answer to that question.

We might take the formal rules of logic to be totally *neutral* with respect to particular features of *objects*. Laws of logic, on this view, must abstract away from particular features of objects. Logic is formal in that it is totally *general*. One way to characterise what counts as a totally *general notion* is by way of permutations. Tarski proposed (1986) that an operation or predicate on a domain counted as general (or logical) if it was invariant under permutations of objects. (A permutation of a collection of objects assigns for each object a unique object in that collection, such that no object is assigned more than once. A permutation of {*a*, *b*, *c*, *d*} might, for example, assign *b* to *a*, *d* to *b*, *c* to *c* and *a* to *d*.) A *2*\-place predicate *R* is invariant under permutation if for any permutation *p*, whenever *Rxy* holds, *Rp*(*x*)*p*(*y*) holds too. You can see that the *identity* relation is permutation invariant—if*x = y* then *p*(*x*) = *p*(*y*)—but the *mother-of* relation is not. We may have permutations *p* such that even though *x* is the mother of *y*, *p*(*x*) is not the mother of *p*(*y*). We may use permutation to characterise logicality for more than predicates too: we may say that a one-place sentential connective ‘•’ is permutation invariant if and only if *p*(•*A*) is true if and only if •*p*(*A*) is true. Defining this rigorously requires establishing how permutations operate on sentences, and this takes us beyond the scope of this article. Suffice to say, an operation such as negation passes the test of invariance, but an operation such as ‘JC believes that’ fails.

A closely related analysis for formality is that formal rules are totally *abstract*. They abstract away from the semantic *content* of thoughts or claims, to leave only semantic structure. The terms ‘mother’ and ‘cousin’ enter essentially into argument (5). On this view, expressions such as propositional connectives and quantifiers do not add new semantic content to expressions, but instead add only ways to combine and structure semantic content. Expressions like ‘mother’ and ‘cousin’, by contrast, add new semantic content.

Another way to draw the distinction (or to perhaps to draw a different distinction) is to take the formal rules of logic to be *constitutitive norms* for thought, regardless of its subject matter. It is plausible to hold that no matter what we think about, it makes sense to conjoin, disjoin and negate our thoughts to make new thoughts. It might also make sense to quantify. The behaviour, then, of logical vocabulary may be used to structure and regulate *any* kind of theory, and the norms governing logical vocabulary apply totally universally. The norms of valid argument, on this picture, are those norms that apply to thought irrespective of the particular content of that thought.

(This section owes much to the work of John MacFarlane, and his thesis [*What Does it Mean to Say that Logic is Formal?*][7] MacFarlane distinguished the three kinds of formality at which we have merely waved here, and he provides a detailed discussion of the notions, making many distinctions over which we have passed.)

## 3\. Proofs and Models

Twentieth Century *technical* work on the notion of logical consequence has centered on two different techniques, one explaining the notion in terms of *proofs* and the other via *models*.

On the *proof-centered approach* to logical consequence, the validity of an argument amounts to there being a *proof* of the conclusions from the premises. Exactly what proofs *are* is a big issue but the idea is fairly plain (at least if you have been exposed to some proof system or other). Proofs are made up of small steps, the primitive inference principles of the proof system. The 20th Century has seen very many different kinds of proof system, from so-called Hilbert proofs, with simple rules and complex axioms, to natural deduction systems, with few (or even no) axioms and very many rules. In natural deduction proofs, the rules are plausibly thought to somehow constitute (or display) the meaning of the connectives. For example, the rules for conjunction dictate that a conjunction *A & B* may be inferred from both conjuncts *A* and *B*, and conversely, from *A & B* one may infer both *A* and *B*. The universal quantifier rules tell us that from the universally quantified claim ∀*x**Fx* we can infer any instance *Fa*, and we can infer ∀*x**Fx* from the instance *Fa*, provided that no other assumption has been made involving the name *a*.

The *model-centered approach* to logical consequence takes the validity of an argument to be *absence of counterexample*. A *counterexample* to an argument is, in general, some way of manifesting the manner in which the premises of the argument *fail* to lead to a conclusion. One way to do this is to provide an argument of the same form for which the premises are clearly true and the conclusion is clearly false. Another way to do this is to provide a *circumstance* in which the premises are true and the conclusion is false. In the contemporary literature the intuitive idea of a counterexample is developed into a theory of *models*. Models are abstract mathematical structures that provide possible interpretations for each of the non-logical primitives in a formal language. Given a model for a language one is able to define what it is for a sentence in that language to be true (according to that model) or not. So, the intuitive idea of logical consequence in terms of counterexamples is then formally rendered as follows: an argument is valid if and only if there is no model according to which the premises are true and the conclusion is not true. Put in positive terms: in any *model* in which the premises are true (or in any *interpretation* of the premises according to which they are true), the conclusion is true too. (This is Tarski's definition of logical consequence from 1936.) Here, the behavior of the logical vocabulary is explained by their (recursive) truth or satisfaction conditions relative to a model. A conjunction, for example, is true in a model if and only if both conjuncts are true in that model. A universally quantified sentence ∀*x**Fx* is true in a model if and only if each instance is true in the model. (Or, on the Tarskian account of satisfaction, if and only if the open sentence *Fx* is satisfied by every object in the domain of the model.) The distinctive logical vocabulary is purely formal, on this picture, because no matter what we say about the semantics of the non-logical parts of the vocabulary, we can determine the truth (or satisfaction) of complex formulas involving conjunction, quantifiers, etc., without knowing anything else about the domain or model. (For detail on how this is accomplished, see the entry on [Tarski's truth definitions][8].)

Just as one can ask after the philosophical import of proof systems, so too one can (and philosophers often do) ask about the philosophical import of the model-centered approach. How, for example, are we to understand the “nature” of models? How are we to understand variation of truth-values across models? John Etchemendy (1990) discusses the philosophical ramifications of taking such variation to be “re-interpretation” of (non-logical) vocabulary versus taking it to reflect variation of “possible worlds”. On one account, models simply model different possible worlds (and so, logical consequence defined by those models is a model of necessary truth preservation). On the other, models provide different interpretations of the non-logical vocabulary of our language (and so, logical consequence is not necessary truth preservation, but rather, truth preservation on the basis of the meanings of the logical vocabulary.)

Once you have two different analyses of a relation of logical consequence, one can ask about what general features such a relation has independently of its analysis as proof-theoretic or model-theoretic. One way of answering this question goes back to [Tarski][9], who introduced the notion of consequence operations. For our purposes, we note only some features of such operations. Let *Cn(X)* be the consequences of *X*. (One can think of the operator *Cn* as deriving from a prior consequence relation which, when taking *X* as ‘input (or premise)’ set, tells you what follows from *X*. But one can also see the ‘process’ in reverse, and a key insight is that consequence relations and corresponding operations are, in effect, interdefinable. See the entry on [propositional consequence relations and algebraic logic][10] for details.) Among some of the minimal conditions one might impose on a consequence relation are the following two (from Tarski):

1.  *X* is a subset of *Cn*(*X*).
2.  *Cn*(*Cn*(*X*)) = *Cn*(*X*).

If you think of *X* as a set of claims, then the first condition tells you that the consequences of a set of claims includes the claims themselves. The second condition demands that the consequences of *X* just are the consequences of the consequences of *X*. Both of these conditions can be motivated from reflection on the model-theoretic and proof-theoretic approaches; and there are other such conditions too. (For a general discussion, see the entry on [propositional consequence relations and algebraic logic][11].) But as with many foundation issues (e.g., ‘what are the essential features of consequence relations in general?’), even such minimal conditions are contentious in philosophical logic and the philosophy of logic. For example, some might take condition (2) to be objectionable on the grounds that, for reasons of vagueness (or more), important consequence relations over natural languages (however formalized) are not generally transitive in ways reflected in (2). (See Cobreros et al 2012, and Ripley 2013, for philosophical motivations against transitive consequence.) But we leave these issues for more advanced discussion.

Conceiving of logical consequence as a relation satisfying structural conditions such as (1) and (2) above (and perhaps other general conditions) brings to light something else shared in the model-theoretic and proof-theoretic definitions of logical consequence. Both of these accounts, at least as typically understood, select from our language a class of expressions, the [logical constants][12], to be treated differently from other expressions in the language. These are interpreted by way of rules of ‘inference’ (in a proof theory) and via fixed truth-in-a-model conditions over all models (in a model theory). An abstract consequence relation can be defined on a language without selecting a particular class of expressions out for special treatment. If we have done so, the question remains: given a particular consequence relation *Cn*, can we recover a set of logical constants in the language? The answer, in many cases, somewhat surprisingly, is a *yes*. Recent work by Dag Westerståhl and Denis Bonnay (Bonnay and Westerståhl 2012, Westerståhl 2012) has shown that we can ‘mine’ a consequence relation in order to single out items in the language of expressions which function as consequence relations, in such a way that were we to define a consequence relation model-theoretically using those expressions, we would get back to the relation we started with. We leave this issue for more advanced discussion (see said work).

The two pictures of logical consequence (viz., proof-theoretic and model-theoretic) are quite different, and they are used for different philosophical purposes. “Realists” typically prefer explaining logical consequence in terms of truth in models, and “Anti-realists” typically prefer explaining logical consequence in terms of proof. There are different reasons for these preferences. Explaining logical consequence in terms of truth in models is rather close to explaining logical consequence in terms of *truth*, and the analysis of truth-in-a-model is sometimes taken to be an explication of truth in terms of correspondence, a typically Realist notion. On the other hand, explaining logical consequence in a proof-centred way seems to require none of this. If the analysis of logical consequence starts with a definition of proof in terms of simple inference rules, then it seems like an attractive possibility to take these inference principles as basic—as *definitions* of the terms involved. If this kind of strategy is successful, then it seems that one is able to give an account of logical consequence in terms acceptable to the Anti-realist, who eschews taking truth (or at least, correspondence-truth) as an explanatory notion. This approach has proponents as different as Prawitz (1985) and Brandom (1994).

While the philosophical divide between Realists and Anti-realists remains vast, proof-centered and model-centered accounts of consequence have been united (at least with respect to extension) in many cases. The great soundness and completeness theorems for different proof systems (or, from the other angle, for different model-theoretic semantics) show that, in an important sense, the two approaches coincide, at least in extension. Intuitively, if soundness and completeness have been established for a particular proof system and a given model-centered account of consequence, then the two accounts agree with each other: there is a proof of an argument if and only if there is no counterexample to it. On the other hand, *extensional* agreement does not make for the same *notion* of consequence, and the cases in which such agreement can be achieved comes at expressive costs. The full philosophical significance of so-called adequacy results (soundness, completeness) remains an open issue.

## 4\. Premises and Conclusions

There has also been dissent, even in Aristotle's day, as to the “shape” of logical consequence. In particular, there is no settled consensus on the number of premises or conclusions appropriate to “tie together” the consequence relation.

In Aristotle's syllogistic, a syllogism relates a pair of premises (the major premise and the minor premise) and a single conclusion. No other kinds of arguments are countenanced. This is clearly a narrowing of a wider notion of logical consequence. If, for example, we have one syllogism from two premises *A* and *B* to a conclusion *C*, and we have another from the premises *C* and *D* to the conclusion *E*, then in some sense, the longer argument from premises *A*, *B* and *D* to conclusion *E* is a good one. It is found by chaining together the two smaller arguments. If the two original arguments are formally valid, then so too is the longer argument from three premises. Aristotle's definition of syllogism also rules out *one*\-premise arguments, including his own “conversion” inferences.

For such reasons, many have taken the relation of logical consequence to pair an arbitrary (possibly infinite) *collection* of premises with a single conclusion. This account has the added virtue of having the special case of an empty collection of premises. Arguments to a conclusion from no premises whatsoever are those in which the conclusion is true by logic alone. Such “conclusions” are *logical truths* (sometimes *tautologies*) or, on the proof-centered approach, *theorems*.

Perhaps there is a reason to allow the notion of logical consequence to apply even more broadly. In Gentzen's proof theory for classical logic, a notion of consequence is defined to hold between multiple premises and multiple conclusions. The argument from a set *X* of premises to a set *Y* of conclusions is valid if the truth of every member of *X* guarantees (in the relevant sense) the truth of some member of *Y*. There is no doubt that this is formally perspicuous, but the philosophical applicability of the multiple premise—multiple conclusion sense of logical consequence remains an open philosophical issue. In particular, those anti-Realists who take logical consequence to be defined in terms of *proof* (such as Michael Dummett) reject a multiple conclusion analysis of logical consequence. For an Anti-realist, who takes good inference to be characterised by the way *warrant* is transmitted from premise to conclusion, it seems that a multiple conclusion analysis of logical consequence is out of the question. In a multiple conclusion argument from *A* to *B*, *C*, any warrant we have for *A* does not necessarily transmit to *B* or *C*: the only conclusion we are warranted to draw is the disjunction *B or C*, so it seems for an analysis of consequence in terms of warrant we need to understand some logical vocabulary (in this case, disjunction) in order to understand the consequence relation. This is unacceptable if we hope to use logical consequence as a tool to *define* that logical vocabulary. No such problems appear to arise in a single conclusion setting. (However, see Restall (2005) for a defence of multiple conclusion consequence for Anti-realists; and see Beall (2011) for a defence of certain sub-classical multiple-conclusion logics in the service of non-classical solutions to paradox.)

Another line along which the notion has been broadened (or along which some have sought to broaden it) involves recent work on [substructural logic][13]. The proposal here is that we may consider doing without some of the standard rules governing the way that premises (or conclusions) of an argument may be combined. Structural rules deal with the *shape* or *structure* of an argument in the sense of the way that the premises and conclusions are collected together, and not the way that those statements are constructed. The structural rule of *weakening* for example, states that if an argument from some collection of premises *X* to a conclusion *C* is valid, then the argument from *X* together with another premise *A* to the conclusion *C* is also valid. This rule has seemed problematic to some (chiefly on the grounds that the extra premise *A* need not be used in the derivation of the conclusion *C* and hence, that *C* does not follow *from* the premises *X,A* in the appropriate sense). [*Relevant logics*][14] are designed to respect this thought, and do without the structural rule of weakening. (For the proof-theoretic picture, see Negri and von Plato (2001).)

Other structural rules are also a called into question. Another possible application of substructural logic is found in the analysis of paradoxes such as [Curry's paradox][15]. A crucial move in the reasoning in Curry's paradox and other paradoxes like it seems to require the step reducing two applications of an assumption to a single one (which is then discharged). According to some, this step is problematic, and so, they must distinguish an argument from *A* to *B* and an argument from *A*, *A* to *B*. The rule of *contraction* is rejected.

In yet other examples, the *order* in which premises are used is important and an argument from *A*, *B* to *C* is to be distinguished from an argument from *B*, *A* to *C*. (For more details, consult the entry on [substructural logics][16].) There is no doubt that the formal systems of substructural logics are elegant and interesting, but the case for the philosophical importance and applicability of substructural logics is not closed.

## 5\. One or Many?

We have touched only on a few central aspects of the notion of logical consequence, leaving further issues, debates and, in particular, details to emerge from particular accounts (accounts that are well-represented in this encyclopedia). But even a quick glance at the *related links* section (below) will attest to a fairly large number of different logical theories, different accounts of what (logically) follows from what. And that observation raises a question with which we will close: Is there one notion of logical consequence that is the target of all such theories, or are there many?

We all agree that there are many different formal techniques for studying logical consequence, and very many different formal systems that each propose different relations of logical consequence. But given a particular argument, is the question as to whether it is deductively valid an all-or-nothing affair? The orthodoxy, logical *monism*, answers affirmatively. There is one relation of deductive consequence, and different formal systems do a better or worse job of modelling that relation. (See, for example, Priest 1999 for a defence of monism.) The logical *contextualist* or *relativist* says that the validity of an argument depends on the subject matter or the frame of reference or some other context of evaluation. (For example, a use of the law of the excluded middle might be valid in a classical mathematics textbook, but not in an intuitionistic mathematics textbook, or in a context where we reason about fiction or vague matters.) The logical *pluralist*, on the other hand, says that of one and the same argument, in one and the same context, there are sometimes different things one should say with respect to its validity. For example, perhaps one ought say that the argument from a contradictory collection of premises to an unrelated conclusion is *valid* in the sense that in virtue of its form it is not the case that the premises are true an the conclusion untrue (so it is valid in one precise sense) but that nonetheless, in another sense the form of the argument does not ensure that the truth of the premises *leads to* the truth of the conclusion. The monist or the contextualist holds that in the case of the one argument a single answer must be found for the question of its validity. The pluralist denies this. The pluralist holds that the notion of logical consequence itself may be made more precise in more than one way, just as the original idea of a “good argument” bifurcates into deductive and inductive validity (see Beall and Restall 2000 for a defence of pluralism).

[1]: https://plato.stanford.edu/archives/fall2014/entries/logic-inductive/
[2]: https://plato.stanford.edu/archives/fall2014/entries/logic-nonmonotonic/
[3]: https://plato.stanford.edu/archives/fall2014/entries/analytic-synthetic/
[4]: https://plato.stanford.edu/archives/fall2014/entries/schema/
[5]: https://plato.stanford.edu/archives/fall2014/entries/logical-form/
[6]: https://plato.stanford.edu/archives/fall2014/entries/logical-constants/
[7]: https://plato.stanford.edu/archives/fall2014/entries/logical-consequence/#macf
[8]: https://plato.stanford.edu/archives/fall2014/entries/tarski-truth/
[9]: https://plato.stanford.edu/archives/fall2014/entries/tarski/
[10]: https://plato.stanford.edu/archives/fall2014/entries/consequence-algebraic/
[11]: https://plato.stanford.edu/archives/fall2014/entries/consequence-algebraic/
[12]: https://plato.stanford.edu/archives/fall2014/entries/logical-constants/
[13]: https://plato.stanford.edu/archives/fall2014/entries/logic-substructural/
[14]: https://plato.stanford.edu/archives/fall2014/entries/logic-relevance/
[15]: https://plato.stanford.edu/archives/fall2014/entries/curry-paradox/
[16]: https://plato.stanford.edu/archives/fall2014/entries/logic-substructural/
